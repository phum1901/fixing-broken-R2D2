{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from src.data import CreateDataset\n",
    "from src.models import Model\n",
    "import re\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path ='Star.Wars.Episode.IV.srt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogue(file_path):\n",
    "    \"\"\"\n",
    "    Extracts dialogue from an SRT file and returns it as a list of strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        srt = f.read()\n",
    "\n",
    "    # Split the SRT into individual subtitle blocks\n",
    "    blocks = srt.strip().split('\\n\\n')\n",
    "\n",
    "    # Extract the dialogue from each subtitle block\n",
    "    dialogue = []\n",
    "    for block in blocks:\n",
    "        # Remove any tags or timestamps from the subtitle block\n",
    "        block = re.sub('<.*?>', '', block)\n",
    "        block = re.sub('\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', block)\n",
    "        block = block.split('\\n\\n')[1:]\n",
    "\n",
    "        dialogue.append(''.join(block))\n",
    "    \n",
    "\n",
    "    return '\\n\\n'.join(dialogue) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"',-.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "dialogues = extract_dialogue(file_path) \n",
    "chars = sorted(list(set(dialogues)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10: 100%|██████████| 1388/1388 [00:07<00:00, 191.46it/s, train_loss=2.23]\n",
      "100%|██████████| 347/347 [00:00<00:00, 418.48it/s, val_loss=2.02]\n",
      "Epoch 1/10: 100%|██████████| 1388/1388 [00:07<00:00, 193.85it/s, train_loss=1.76]\n",
      "100%|██████████| 347/347 [00:00<00:00, 418.50it/s, val_loss=1.91]\n",
      "Epoch 2/10: 100%|██████████| 1388/1388 [00:07<00:00, 194.55it/s, train_loss=1.6] \n",
      "100%|██████████| 347/347 [00:00<00:00, 427.53it/s, val_loss=1.87]\n",
      "Epoch 3/10: 100%|██████████| 1388/1388 [00:07<00:00, 193.81it/s, train_loss=1.51]\n",
      "100%|██████████| 347/347 [00:00<00:00, 414.86it/s, val_loss=1.88]\n",
      "Epoch 4/10: 100%|██████████| 1388/1388 [00:07<00:00, 190.89it/s, train_loss=1.45]\n",
      "100%|██████████| 347/347 [00:00<00:00, 415.95it/s, val_loss=1.9] \n",
      "Epoch 5/10: 100%|██████████| 1388/1388 [00:07<00:00, 192.48it/s, train_loss=1.41]\n",
      "100%|██████████| 347/347 [00:00<00:00, 453.83it/s, val_loss=1.91]\n",
      "Epoch 6/10:  14%|█▍        | 201/1388 [00:01<00:07, 161.16it/s, train_loss=1.39]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m xb, yb \u001b[39m=\u001b[39m xb\u001b[39m.\u001b[39mto(device), yb\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m logits \u001b[39m=\u001b[39m model(xb)\n\u001b[1;32m     44\u001b[0m \u001b[39m# loss\u001b[39;00m\n\u001b[1;32m     45\u001b[0m b, s, c \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/nameit/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nameit/src/models.py:90\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m pos_embd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding(\n\u001b[1;32m     87\u001b[0m     torch\u001b[39m.\u001b[39marange(s, device\u001b[39m=\u001b[39midx\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     88\u001b[0m )  \u001b[39m# (b, s, n_embd)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m x \u001b[39m=\u001b[39m token_embd \u001b[39m+\u001b[39m pos_embd  \u001b[39m# (b, s, n_embd)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x)  \u001b[39m# (b, s, head_size)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(x)\n\u001b[1;32m     92\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/nameit/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nameit/src/models.py:44\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 44\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x)\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/nameit/src/models.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 44\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x)\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/nameit/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nameit/src/models.py:21\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m b, s, c \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape  \u001b[39m# (batch, seq_length, channels)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(x)  \u001b[39m# (b, s, c) --> (b, s, head_size)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(x)  \u001b[39m# (b, s, c) --> (b, s, head_size)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m attn \u001b[39m=\u001b[39m (\n\u001b[1;32m     24\u001b[0m     q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhead_size\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[1;32m     25\u001b[0m )  \u001b[39m# (b, s, head_size) @ (b, head_size, s) --> (b, s, s)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39mmasked_fill(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtril[:s, :s] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/nameit/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nameit/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(555)\n",
    "args = argparse.Namespace\n",
    "args.batch_size = 32\n",
    "args.vocab_size = vocab_size\n",
    "args.seq_length = 32\n",
    "args.n_embd = 64\n",
    "args.head_size = 16\n",
    "args.n_head = 4  \n",
    "args.epochs = 10\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = CreateDataset(dialogues, seq_length=args.seq_length, size=0.8)\n",
    "train_ds = dataset.train_dataset()\n",
    "val_ds = dataset.test_dataset()\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=args.batch_size, shuffle=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# model\n",
    "model = Model(args).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = args.epochs\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ## train  \n",
    "    # progress bar\n",
    "    pbar = tqdm(train_dl)\n",
    "\n",
    "    # running loss\n",
    "    mloss = torch.zeros(1, device=device)\n",
    "    for ib, (xb, yb) in enumerate(pbar): \n",
    "        model.train()\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        # forward\n",
    "        logits = model(xb)\n",
    "\n",
    "        # loss\n",
    "        b, s, c = logits.shape\n",
    "        logits.shape, yb.shape\n",
    "        logits = logits.view(b*s, -1)\n",
    "        loss = F.cross_entropy(logits, yb.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(loss.item())\n",
    "\n",
    "        mloss = (ib * mloss + loss)/(ib + 1)\n",
    "\n",
    "        # prograss bar\n",
    "        pbar.set_description(f'Epoch {epoch}/{epochs}')\n",
    "        pbar.set_postfix(train_loss=mloss.item())\n",
    "    \n",
    "\n",
    "    ## validation\n",
    "    # prograss bar\n",
    "    pbar = tqdm(val_dl)\n",
    "\n",
    "    # running loss\n",
    "    mloss = torch.zeros(1, device=device) \n",
    "    for ib, (xb, yb) in enumerate(pbar):\n",
    "        model.eval()\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # forward\n",
    "            logits = model(xb)\n",
    "\n",
    "            # loss\n",
    "            b, s, c = logits.shape\n",
    "            logits.shape, yb.shape\n",
    "            logits = logits.view(b*s, -1)\n",
    "            loss = F.cross_entropy(logits, yb.view(-1))\n",
    "            mloss = (ib * mloss + loss) / (ib + 1)\n",
    "\n",
    "        pbar.set_postfix(val_loss=mloss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Wars anvoyand.\n",
      "\n",
      "Ninow. You'll like me.\n",
      "\n",
      "Get in talk in.\n",
      "\n",
      "Tell? Your funning battled.\n",
      "\n",
      "Are my betiling.\n",
      "\n",
      "I'll man does of functions\n",
      "\n",
      "and grew nin.\n",
      "\n",
      "There's nould ank are go you'd think were your smore her andpeople.\n",
      "\n",
      "I gonnowning was to kay he's get.\n",
      ". No, come pilons.\n",
      "\n",
      "Look to you kill tran\n",
      "where you!\n",
      "\n",
      "R2. Yough onf, the chort train'?\n",
      "\n",
      "With this Oh.\n",
      "\n",
      "Ai, and a grouth tyour uncomise.\n",
      "\n",
      "How nitsation.\n",
      "\n",
      "Where any, are the short so star back\n",
      "out it.\n",
      "Come off.\n",
      "\n",
      "Grrr. Princ blast smanning.\n",
      "\n",
      "Hange this?\n",
      "\n",
      "Pam pups weapod and, Our Dunctic\n",
      "with yet.\n",
      "\n",
      "I takeind\n",
      "\n",
      "to of be the dam and lations.\n",
      "\n",
      "The ould gone ship. Settinifical.\n",
      "\n",
      "We be on to hidner.\n",
      "\n",
      "Ninow.\n",
      "\n",
      "Don't get man a firide a pies, figurap in the please sust stated frouble rightis it.\n",
      "\n",
      "We hath somes. Over relanced.\n",
      "\n",
      "Okay, Master, R2 uniten betranspor deturech ording back canneverser.\n",
      "\n",
      "Yegenerself the samend is a migg.\n",
      "\n",
      "\n",
      "The Ann their not very pizale me.\n",
      "\n",
      "It's in your cometimes were quite Alderaan\n",
      "before timalfunctions.\n",
      "\n",
      "Grrr in to tir.\n",
      "\n",
      "Where \n"
     ]
    }
   ],
   "source": [
    "idx = torch.randint(high = args.vocab_size, size=(16, 1), dtype=torch.int64, device=device) \n",
    "model(idx).shape\n",
    "idxs = model.generate(idx, 1000)\n",
    "\n",
    "print([dataset.decode(i) for i in idxs.tolist()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, H)\n",
    "q = query(x) # (B, T, H)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, H) @ (B, H, T) --> (B, T, T) \n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# # wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "args = argparse.Namespace\n",
    "args.batch_size = 4\n",
    "args.seq_length = 8\n",
    "args.n_embd = 32\n",
    "args.head_size = 16\n",
    "args.n_head = 2\n",
    "x = torch.randn(args.batch_size, args.seq_length, args.n_embd)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, args) -> None:\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.key = nn.Linear(args.n_embd, args.head_size, bias=False)\n",
    "        self.query = nn.Linear(args.n_embd, args.head_size, bias=False)\n",
    "        self.value = nn.Linear(args.n_embd, args.head_size, bias=False)\n",
    "        # self.tril = torch.tril(torch.ones((block_size, block_size), requires_grad=False))\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((args.seq_length, args.seq_length))))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, s, c = x.shape # (batch, seq_length, channels) \n",
    "        k = self.key(x) # (b, s, c) --> (b, s, head_size)\n",
    "        q = self.query(x) # (b, s, c) --> (b, s, head_size)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1) * self.args.head_size ** -0.5 # (b, s, head_size) @ (b, head_size, s) --> (b, s, s)\n",
    "\n",
    "        attn = attn.masked_fill(self.tril[:s, :s] == 0, float('-inf')) \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        v = self.value(x) # (b, s, c) --> (b, s, head_size) \n",
    "        out = attn @ v # (b, s, s) @ (b, s, head_size) --> (b, s, head_size) \n",
    "        return out \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size, n_embedding, block_size) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size, n_embedding, block_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_embedding, n_embedding, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_embedding, 4 * n_embedding)\n",
    "        self.l2 = nn.Linear(4 * n_embedding, n_embedding)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_head, n_embedding, block_size) -> None:\n",
    "        super().__init__()\n",
    "        head_size = n_embedding // n_head \n",
    "        self.sa = MultiHeadAttention(n_head=n_head, head_size=head_size, n_embedding=n_embedding, block_size=block_size)\n",
    "        self.ffwd = FeedForward(n_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "head = AttentionHead(args)\n",
    "head(x).shape\n",
    "\n",
    "# sum([p.numel() for p in head.parameters()])\n",
    "# multihead = MultiHeadAttention(n_head=n_head, head_size=head_size, n_embedding=n_embedding, block_size=block_size)\n",
    "# ffwd = FeedForward(n_embedding)\n",
    "# ffwd(x)[0]\n",
    "# decoder_block = DecoderBlock(n_head=n_head, n_embedding=n_embedding, block_size=block_size)\n",
    "# decoder_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.7861e-02, -3.6411e-01, -9.2836e-02,  1.0961e-01, -1.4164e+00,\n",
       "           4.9022e-01, -9.6439e-02, -2.9878e-01,  6.7750e-01,  2.6803e-01,\n",
       "           7.8892e-02,  1.3635e-01,  1.7553e-02,  5.4448e-01, -3.9418e-01,\n",
       "           2.8714e-01],\n",
       "         [-5.2276e-01, -4.9407e-01,  4.2566e-01,  1.4612e-01, -9.2816e-01,\n",
       "           5.3669e-02,  5.4877e-01,  2.8532e-01,  5.8181e-01,  1.1234e-01,\n",
       "          -3.8405e-01,  1.9452e-01, -1.2907e-01, -5.3947e-01,  2.5659e-01,\n",
       "          -9.2728e-02],\n",
       "         [-4.3416e-01, -3.5311e-01, -2.2315e-02,  4.5016e-01, -6.0007e-01,\n",
       "           3.6870e-02,  2.2213e-01,  1.7629e-02,  2.7614e-01,  1.4560e-01,\n",
       "          -6.7048e-01, -1.1931e-01,  9.5055e-02, -1.4623e-02, -4.2846e-02,\n",
       "           2.8831e-01],\n",
       "         [-4.1676e-01, -3.0557e-01, -7.8429e-02,  5.6775e-01, -4.8637e-01,\n",
       "           3.3208e-02,  2.2359e-01, -3.6059e-03,  2.0053e-01,  2.7256e-02,\n",
       "          -8.1671e-01, -1.9872e-01,  9.0726e-02, -2.6008e-02, -8.1995e-02,\n",
       "           3.8695e-01],\n",
       "         [-2.5422e-01, -1.4030e-01,  5.7450e-03,  4.7646e-01, -5.0909e-01,\n",
       "           1.3964e-01,  2.0858e-01,  4.6030e-02,  1.1994e-01, -1.8464e-01,\n",
       "          -5.5716e-01, -5.9170e-02,  3.3986e-02, -9.3892e-02, -1.1335e-01,\n",
       "           3.2347e-01],\n",
       "         [-2.5710e-01, -4.1822e-01,  3.5727e-01,  5.0626e-02, -4.6278e-01,\n",
       "           3.2724e-01,  4.9477e-01,  1.5104e-01,  1.7127e-01, -3.5465e-01,\n",
       "          -2.0291e-01,  6.1174e-02, -2.9702e-03, -3.3570e-01, -4.4470e-01,\n",
       "           1.7337e-02],\n",
       "         [-1.7615e-01, -3.0210e-01,  1.7422e-01,  2.0081e-01, -2.6371e-01,\n",
       "           2.9022e-01,  1.8653e-01,  1.2997e-01,  6.3009e-02, -2.8562e-01,\n",
       "          -1.5041e-01,  2.3552e-02,  1.4684e-01, -1.3623e-01, -3.9191e-01,\n",
       "           1.5505e-02],\n",
       "         [-7.2620e-02, -2.1168e-01,  2.4126e-01,  2.3783e-01, -1.4081e-01,\n",
       "           2.4470e-01,  1.2333e-01,  1.9619e-01,  3.0000e-02, -2.9346e-01,\n",
       "          -2.4727e-02,  1.7778e-01,  1.6146e-01, -2.4492e-01, -2.2283e-01,\n",
       "          -1.1368e-02]],\n",
       "\n",
       "        [[ 3.9846e-01, -7.2508e-02, -8.4599e-01, -1.8148e-01,  2.7540e-01,\n",
       "          -5.1514e-01, -9.3578e-01,  2.3450e-01, -1.1556e+00,  1.9587e-01,\n",
       "           9.6449e-02, -1.3320e-01,  4.0755e-01,  3.3152e-01, -5.8905e-01,\n",
       "           4.3694e-01],\n",
       "         [ 1.2513e-01, -1.5469e-03, -4.8565e-01, -2.1421e-01,  5.7968e-01,\n",
       "          -2.2528e-01, -5.3697e-01,  2.7921e-01, -7.9250e-01, -4.6352e-02,\n",
       "           9.0720e-02, -2.3642e-01,  1.7625e-01,  3.2306e-01, -4.2255e-01,\n",
       "          -2.0791e-02],\n",
       "         [ 1.8004e-01,  1.0811e-01,  1.2393e-01,  2.7688e-02,  4.9724e-01,\n",
       "           1.3560e-01, -3.8337e-01,  5.2616e-01, -5.7139e-01, -3.6970e-01,\n",
       "           1.1186e-01, -1.1252e-01,  2.8131e-02, -1.9279e-01, -1.8893e-01,\n",
       "          -8.0047e-01],\n",
       "         [ 1.6989e-01, -5.0506e-02, -3.2144e-01, -1.1621e-01,  2.7034e-01,\n",
       "           1.6189e-01, -5.3389e-01,  9.9003e-02, -7.6729e-01, -3.2929e-01,\n",
       "           8.7119e-02, -1.7338e-01,  2.3500e-01,  1.3659e-01, -5.7988e-01,\n",
       "          -2.9037e-01],\n",
       "         [ 1.5769e-01, -6.9862e-02, -9.7902e-03, -2.7687e-02,  4.5250e-01,\n",
       "           1.4618e-01, -4.4520e-01,  4.8213e-01, -4.7204e-01, -2.9246e-01,\n",
       "           3.3177e-01, -4.0370e-02, -1.9247e-02, -4.2542e-01, -2.5790e-01,\n",
       "          -7.5242e-01],\n",
       "         [ 7.3785e-02, -1.3778e-01, -3.5483e-01, -1.2790e-01,  3.0137e-01,\n",
       "           1.7163e-01, -4.7681e-01,  1.1080e-01, -5.8667e-01, -1.7094e-01,\n",
       "           2.0176e-01, -1.8842e-01,  2.1035e-01, -1.1305e-01, -5.4378e-01,\n",
       "          -3.0569e-01],\n",
       "         [-9.3870e-02, -1.5353e-01, -1.5547e-01, -3.3626e-02,  2.2172e-01,\n",
       "           1.1895e-01, -1.5921e-01,  2.1437e-01, -3.2043e-01, -4.3543e-02,\n",
       "           1.2102e-01, -2.4824e-01,  1.0808e-01, -3.2600e-01, -2.8293e-01,\n",
       "          -3.8240e-01],\n",
       "         [-1.7817e-01, -1.9910e-01, -5.5761e-02, -7.1125e-03, -1.8562e-01,\n",
       "           1.5136e-01,  1.6541e-01,  7.1338e-02, -2.3584e-01, -1.1895e-01,\n",
       "          -3.1575e-02, -2.2742e-01,  1.5656e-01, -2.7772e-01, -2.0538e-01,\n",
       "          -2.3021e-01]],\n",
       "\n",
       "        [[ 9.2403e-01,  4.4965e-01, -6.9508e-01, -1.0204e+00,  2.9634e-01,\n",
       "           1.6488e-01, -3.6231e-01,  8.6457e-01,  2.1268e-01,  4.9765e-03,\n",
       "           6.4351e-01,  5.4267e-01,  5.5433e-01, -8.0138e-01,  3.8619e-01,\n",
       "          -1.0801e+00],\n",
       "         [ 2.0372e-01, -3.0041e-01, -4.4565e-01,  6.0444e-02,  2.6293e-01,\n",
       "          -1.4768e-01, -9.2138e-02,  3.2502e-02,  3.5564e-01, -4.3773e-02,\n",
       "           2.2168e-01, -1.6648e-01, -6.3952e-02,  3.9219e-03,  7.7136e-02,\n",
       "          -4.6393e-02],\n",
       "         [ 1.9816e-01, -2.6818e-01, -4.1721e-01,  2.5763e-02,  2.2543e-01,\n",
       "           8.8859e-02, -2.4783e-01,  1.5052e-01,  3.3909e-01,  2.9653e-02,\n",
       "           3.0166e-01,  1.5450e-01,  1.7931e-01,  3.7134e-03, -2.6204e-02,\n",
       "          -2.5200e-01],\n",
       "         [-2.0218e-01, -3.6874e-01, -2.3833e-01,  2.8502e-01,  2.1347e-01,\n",
       "          -4.5438e-02, -1.3763e-01, -8.7590e-03,  3.4113e-01, -4.3853e-02,\n",
       "           1.6628e-01,  1.1356e-01,  3.0712e-02,  1.0352e-01, -1.2204e-01,\n",
       "          -2.4892e-01],\n",
       "         [-2.0981e-01, -1.4079e-01,  5.7174e-02,  2.1919e-01,  1.8392e-02,\n",
       "          -1.3825e-01, -1.3095e-01, -1.6868e-01,  1.9527e-01, -1.2323e-01,\n",
       "           2.9682e-02,  2.4538e-01,  1.2477e-01, -4.9145e-03, -4.7711e-02,\n",
       "          -9.1409e-02],\n",
       "         [-1.2730e-01, -3.3399e-05,  2.2288e-01,  3.3883e-01,  2.7539e-02,\n",
       "           3.2489e-02, -2.0027e-01, -1.7361e-01,  2.8897e-01,  3.1984e-02,\n",
       "           7.9660e-02,  4.4670e-01,  1.5078e-01,  1.2486e-01,  7.0258e-02,\n",
       "          -8.7732e-02],\n",
       "         [-7.4786e-02,  8.3116e-02,  2.1721e-01,  2.6355e-01,  4.4977e-02,\n",
       "          -3.7994e-02, -2.0628e-01, -6.8647e-02,  2.2754e-01, -4.7929e-02,\n",
       "           1.6221e-01,  3.8811e-01,  8.8787e-02, -7.2131e-02,  1.2597e-01,\n",
       "          -2.3732e-01],\n",
       "         [-1.8709e-01,  6.4717e-03,  1.8531e-01,  2.9566e-01, -2.9550e-02,\n",
       "          -1.1445e-01, -1.7000e-01, -1.5479e-01,  2.0296e-01, -6.2839e-02,\n",
       "           1.1240e-01,  3.5420e-01,  4.5508e-02, -1.1774e-01,  1.8023e-01,\n",
       "          -1.1091e-01]],\n",
       "\n",
       "        [[-5.9885e-02,  5.3339e-02, -2.3609e-01,  4.0664e-01,  5.3082e-01,\n",
       "           1.4422e-01,  3.2946e-02,  3.0217e-01,  5.1376e-02,  3.5749e-01,\n",
       "          -9.8740e-02, -2.4214e-01,  8.2812e-01, -6.9981e-01, -4.9654e-01,\n",
       "          -4.4661e-01],\n",
       "         [ 3.6407e-01,  4.2900e-01, -5.4275e-01,  3.4465e-01,  3.1128e-01,\n",
       "           1.3790e-01, -4.5739e-02, -1.5653e-02,  1.7876e-02,  1.7510e-02,\n",
       "           3.1615e-02, -4.6469e-01,  2.2573e-01, -1.9520e-01, -1.5977e-01,\n",
       "          -1.9574e-01],\n",
       "         [ 2.6947e-02,  3.1829e-01, -4.0753e-01,  2.0217e-01, -1.4283e-02,\n",
       "          -3.0972e-02, -1.0173e-01, -2.4186e-01,  2.9596e-01,  1.7894e-01,\n",
       "          -3.4339e-01, -2.6979e-01,  5.2136e-01, -4.1009e-01, -1.6167e-01,\n",
       "           4.6077e-02],\n",
       "         [ 2.6862e-02,  1.7732e-01, -3.0195e-01,  1.4788e-01, -3.1951e-01,\n",
       "           4.7604e-02, -4.2700e-02, -2.3607e-01,  4.7101e-01,  1.3752e-01,\n",
       "          -4.2804e-01, -7.4944e-02,  4.0121e-01, -6.6440e-01, -8.8257e-02,\n",
       "           4.3247e-02],\n",
       "         [-5.9752e-02,  1.3986e-01, -2.2578e-01,  9.1167e-02, -2.5819e-01,\n",
       "           2.0141e-01,  1.9246e-01, -1.4565e-01,  2.4849e-01,  7.8831e-02,\n",
       "          -2.2904e-01, -9.4690e-02,  4.6231e-01, -5.3206e-01, -1.8967e-01,\n",
       "           9.6662e-03],\n",
       "         [ 1.6363e-02,  2.4947e-01, -3.1663e-01,  1.7668e-01, -1.9356e-01,\n",
       "           2.6912e-01,  1.8484e-01, -7.5418e-02,  1.0646e-01, -1.2291e-01,\n",
       "          -1.2793e-01, -2.5403e-01,  2.7619e-01, -3.6746e-01, -9.8737e-02,\n",
       "          -7.2655e-02],\n",
       "         [-1.8529e-01,  8.3708e-02, -7.6513e-03,  1.5929e-01, -1.5022e-01,\n",
       "           3.7126e-01,  4.6464e-01,  1.2440e-01,  3.0533e-02, -1.8390e-01,\n",
       "          -5.8292e-02, -8.5767e-03,  6.3377e-01, -6.1924e-01, -2.0379e-01,\n",
       "          -1.5330e-01],\n",
       "         [-4.9679e-04,  1.6605e-01, -4.6127e-02,  1.9653e-01, -2.2717e-01,\n",
       "           2.8126e-01,  3.1838e-01,  5.3208e-02,  2.2668e-01, -2.2926e-01,\n",
       "          -2.1316e-01, -2.6240e-02,  5.1289e-01, -6.5934e-01, -3.9421e-02,\n",
       "          -7.4273e-02]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "n_head = 2\n",
    "head_size = 16\n",
    "x = torch.randn(batch_size, block_size, n_embd)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.size(-1)**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd, False)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        # out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            # nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "head = Head(head_size=head_size)\n",
    "# multihead = MultiHeadAttention(num_heads=n_head, head_size=head_size)\n",
    "# ffwd = FeedFoward(n_embd)\n",
    "# ffwd(x)[0]\n",
    "head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
